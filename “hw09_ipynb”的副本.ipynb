{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“hw09.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaLvIbKTky9I"
      },
      "source": [
        "# Homework 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47WUKJmaky9N"
      },
      "source": [
        "## Imports and Utilities\n",
        "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cJDe6AOky9O"
      },
      "source": [
        "from collections import defaultdict\n",
        "from math import sqrt, log\n",
        "import abc\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "\n",
        "class MDP:\n",
        "    \"\"\"A Markov Decision Process.\"\"\"\n",
        "\n",
        "    @property\n",
        "    @abc.abstractmethod\n",
        "    def state_space(self):\n",
        "        \"\"\"Representation of the MDP state set.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @property\n",
        "    @abc.abstractmethod\n",
        "    def action_space(self):\n",
        "        \"\"\"Representation of the MDP action set.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @property\n",
        "    def temporal_discount_factor(self):\n",
        "        \"\"\"Gamma, defaults to 1.\n",
        "        \"\"\"\n",
        "        return 1.\n",
        "\n",
        "    @property\n",
        "    def horizon(self):\n",
        "        \"\"\"H, defaults to inf.\n",
        "        \"\"\"\n",
        "        return float(\"inf\")\n",
        "\n",
        "    def state_is_terminal(self, state):\n",
        "        \"\"\"Designate certain states as terminal (done) states.\n",
        "\n",
        "        Defaults to False.\n",
        "\n",
        "        Args:\n",
        "            state: A state.\n",
        "\n",
        "        Returns:\n",
        "            state_is_terminal : A bool.\n",
        "        \"\"\"\n",
        "        return False\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_reward(self, state, action, next_state=None):\n",
        "        \"\"\"Return (deterministic) reward for executing action\n",
        "        in state.\n",
        "\n",
        "        Args:\n",
        "            state: A current state.\n",
        "            action: An action.\n",
        "            next_state: Optional. A next state.\n",
        "\n",
        "        Returns:\n",
        "            reward : Single time step reward.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_transition_distribution(self, state, action):\n",
        "        \"\"\"Return a distribution over next states.\n",
        "\n",
        "        The form of this distribution will vary, e.g., depending\n",
        "        on whether the MDP has discrete or continuous states.\n",
        "\n",
        "        Args:\n",
        "            state: A current state.\n",
        "            action: An action.\n",
        "\n",
        "        Returns:\n",
        "            next_state_distribution: Distribution over next states.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    def sample_next_state(self, state, action, rng=np.random):\n",
        "        \"\"\"Sample a next state from the transition distribution.\n",
        "\n",
        "        This function may be overwritten by subclasses when the explicit\n",
        "        distribution is too large to enumerate.\n",
        "\n",
        "        Args:\n",
        "            state: A state from the state space.\n",
        "            action: An action from the action space.\n",
        "            rng: A random number generator.\n",
        "\n",
        "        Returns:\n",
        "            next_state: A sampled next state from the state space.\n",
        "        \"\"\"\n",
        "        next_state_dist = self.get_transition_distribution(state, action)\n",
        "        next_states, probs = zip(*next_state_dist.items())\n",
        "        next_state_index = rng.choice(len(next_states), p=probs)\n",
        "        next_state = next_states[next_state_index]\n",
        "        return next_state\n",
        "\n",
        "\n",
        "class POMDP(MDP):\n",
        "    \"\"\"A partially observable Markov decision process (POMDP).\"\"\"\n",
        "\n",
        "    @property\n",
        "    @abc.abstractmethod\n",
        "    def observation_space(self):\n",
        "        \"\"\"Representation of the POMDP observation space.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @abc.abstractclassmethod\n",
        "    def get_observation_distribution(self, next_state, action):\n",
        "        \"\"\"Return a distribution over the observations.\n",
        "\n",
        "        The form of this distribution will vary, e.g., depending\n",
        "        on whether the MDP has discrete or continuous observation\n",
        "        spaces.\n",
        "\n",
        "        Args:\n",
        "            next_state: The next state.\n",
        "            action: The action taken.\n",
        "\n",
        "        Returns:\n",
        "            observation_distribution: Distribution over the observation.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "\n",
        "class LambdaMDP(MDP):\n",
        "    \"\"\"A helper class that creates a MDP class based on a set of functions.\n",
        "    See the constructor for details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_space, action_space, state_is_terminal_fn, get_reward_fn, get_transition_distribution_fn, temporal_discount_factor=1.0):\n",
        "        \"\"\"\n",
        "        Construct a MDP class based on a set of function definitions.\n",
        "\n",
        "        Args:\n",
        "            state_space: The set of possible states.\n",
        "            action_space: The set of possible actions.\n",
        "            state_is_terminal_fn: A callable function: state_is_terminal_fn(state) -> bool,\n",
        "                mapping a state to a boolean value indicating whether\n",
        "                the state is a terminal state.\n",
        "            get_reward_fn: A callable function: get_reward_fn(state, action, next_state) -> float,\n",
        "                mapping a (s, a, s') tuple to a float reward value.\n",
        "            get_transition_distribution_fn: A callable function:\n",
        "                get_transition_distribution_fn(state, action) -> distribution of the next state.\n",
        "                Note that the return value for this function must be a discrete distribution.\n",
        "            temporal_discount_factor: A float number, the temporal discount factor of the MDP.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.state_space_v = state_space\n",
        "        self.action_space_v = action_space\n",
        "        self.state_is_terminal = state_is_terminal_fn\n",
        "        self.get_reward = get_reward_fn\n",
        "        self.get_transition_distribution = get_transition_distribution_fn\n",
        "        self.temporal_discount_factor_v = temporal_discount_factor\n",
        "\n",
        "    @property\n",
        "    def state_space(self):\n",
        "        return self.state_space_v\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return self.action_space_v\n",
        "\n",
        "    @property\n",
        "    def temporal_discount_factor(self):\n",
        "        return self.temporal_discount_factor_v\n",
        "\n",
        "\n",
        "class DiscreteDistribution(object):\n",
        "    \"\"\"A discrete distribution, represneted as a dictionary.\"\"\"\n",
        "\n",
        "    eps = 1e-6\n",
        "\n",
        "    def __init__(self, prob_dict):\n",
        "        \"\"\"Construct a discrete distribution based on a probability dictionary.\n",
        "        The dictionary might be \"sparse\", in which case the omitted entries are\n",
        "        treated as zero-probability values.\n",
        "\n",
        "        Note that, even if the random varaible takes values from a continuous space,\n",
        "        (e.g., all real numbers), we can still define a \"discrete distribution\",\n",
        "        that is, a distribution only has mass on a finite set of points.\n",
        "        For example, we can define a distribution on R: {0: 0.5, 1: 0.5}.\n",
        "        Implicitly, all values not in the prob_dict will be treated as\n",
        "        zero-probability.\n",
        "\n",
        "        Example:\n",
        "\n",
        "        ```\n",
        "        p = DiscreteDistribution({'x': 0.0, 'y': 0.6, 'z': 0.4}, {'x', 'y', 'z'})\n",
        "        print(p.p('x'))  # 0.0\n",
        "        for x in p:  # iterate over the set of possible values.\n",
        "            print(x, p.p(x))  # should print y 0.6 z 0.4\n",
        "        for x, p_x in p.items():  # just like iterating over a Python dict.\n",
        "            print(x, p_x)  # should print y 0.6 z 0.4\n",
        "        ```\n",
        "        Note that, during iteration, zero-probability values will be omitted.\n",
        "\n",
        "        Args:\n",
        "            prob_dict: A dictionary, mapping elements in the domain to a float\n",
        "                number. The dictionary might be sparse. It should always\n",
        "                sum up to one (thus being a valid distribution.)\n",
        "        \"\"\"\n",
        "        self.prob_dict = prob_dict\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Iterate over the support set.\"\"\"\n",
        "        yield from self.support()\n",
        "\n",
        "    def support(self):\n",
        "        \"\"\"Itearte over the support set of the distribution. That is,\n",
        "        values with a non-zero probability mass. \n",
        "        \"\"\"\n",
        "        for k, v in self.prob_dict.items():\n",
        "            if v > 0:\n",
        "                yield k\n",
        "\n",
        "    def items(self):\n",
        "        \"\"\"Iterate over the distribution. Generates a list of (x, p(x)) pairs.\n",
        "        This function will ignore zero-probability values in the prob_dict.\n",
        "        \"\"\"\n",
        "        for k, v in self.prob_dict.items():\n",
        "            if v > 0:\n",
        "                yield k, v\n",
        "\n",
        "    def p(self, value):\n",
        "        \"\"\"Evaluate the proabbility of a value in the support set.\n",
        "\n",
        "        Args:\n",
        "            value: An object in the domain of the distribution.\n",
        "\n",
        "        Returns:\n",
        "            p: A float, indicating p(value). For values not in the support (prob_dict),\n",
        "                the probability is assumed to be zero.\n",
        "        \"\"\"\n",
        "        return self.prob_dict.get(value, 0.)\n",
        "\n",
        "    def renormalize(self):\n",
        "        \"\"\"Renormalize the distribution to ensure that the probabilities sum up to 1.\n",
        "\n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        z = sum(self.prob_dict.values())\n",
        "        assert z > 0, 'Degenerated probability distribution.'\n",
        "        self.prob_dict = {k: v / z for k, v in self.prob_dict.items()}\n",
        "        return self\n",
        "\n",
        "    def check_normalization(self):\n",
        "        \"\"\"Check if the prob dict is correctly normalized (i.e., should sum up to 1).\"\"\"\n",
        "        assert 1 - type(self).eps < sum(self.prob_dict.values()) < 1 + type(self).eps\n",
        "\n",
        "    def max(self):\n",
        "        \"\"\"Return argmax_x p(x).\n",
        "\n",
        "        Returns:\n",
        "            arg_max: An object in the support, argmax_x p(x).\n",
        "        \"\"\"\n",
        "        return max(self.prob_dict, key=self.prob_dict.get)\n",
        "\n",
        "    def draw(self, rng=None):\n",
        "        if rng is None:\n",
        "            rng = np.random\n",
        "        keys = list(self.prob_dict.keys())\n",
        "        probs = [self.prob_dict[k] for k in keys]\n",
        "        return keys[rng.choice(len(keys), p=probs)]\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return str(self.prob_dict)\n",
        "\n",
        "\n",
        "def OnehotDiscreteDistribution(obj):\n",
        "    \"\"\"Create a DiscreteDistribution of p(obj) = 1.\"\"\"\n",
        "    return DiscreteDistribution({obj: 1.0})\n",
        "\n",
        "\n",
        "def UniformDiscreteDistribution(support):\n",
        "    \"\"\"Create a DiscreteDistribution that is uniform. That is, for any object x, p(x) = 1 / |support|.\"\"\"\n",
        "    return DiscreteDistribution({x: 1 / len(support) for x in support})\n",
        "\n",
        "# Our RobotChargingPOMDP\n",
        "\n",
        "class RobotChargingPOMDP(POMDP):\n",
        "    DEF_MOVE_SUCCESS = 0.8\n",
        "    DEF_OBS_IF_THERE = 0.9\n",
        "    DEF_OBS_IF_NOT_THERE = 0.4\n",
        "    DEF_C_MOVE = 0.5\n",
        "    DEF_C_LOOK = 0.1\n",
        "    DEF_GAMMA = 0.9\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        p_move_success=DEF_MOVE_SUCCESS, p_obs_if_there=DEF_OBS_IF_THERE, p_obs_if_not_there=DEF_OBS_IF_NOT_THERE,\n",
        "        c_move=DEF_C_MOVE, c_look=DEF_C_LOOK,\n",
        "        gamma=DEF_GAMMA\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create the Robot Charging POMDP.\n",
        "\n",
        "        Args:\n",
        "            p_move_success (float): the probability that a move action is successful.\n",
        "            p_obs_if_there (float): the probability of return 1 when looking at a location with the charger.\n",
        "            p_obs_if_not_there (float): the probability of return 1 when looking at a location without a charger.\n",
        "            c_move (float): the cost of a move action.\n",
        "            c_look (float): the cost of a look action.\n",
        "            gamma (float): the temporal discount factor.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.p_move_success = p_move_success\n",
        "        self.p_obs_if_there = p_obs_if_there\n",
        "        self.p_obs_if_not_there = p_obs_if_not_there\n",
        "        self.c_move = c_move\n",
        "        self.c_look = c_look\n",
        "        self.gamma = gamma\n",
        "\n",
        "    @property\n",
        "    def state_space(self):\n",
        "        \"\"\"\n",
        "        Three \"normal\" states: 0, 1, 2, indicating the position of the charger.\n",
        "        One \"terminal\" state T. Executing the \"charge\" action will reach this\n",
        "        terminal state. And the state is absorbing. The robot will deterministically\n",
        "        transition to this terminal state when we execute the c action.\n",
        "        \"\"\"\n",
        "        return {0, 1, 2, 'T'}\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        # lx: look(x)\n",
        "        # mxy: move(start=x, target=y)\n",
        "        # c: charge\n",
        "        # nop: NOP\n",
        "        return {'l0', 'l1', 'l2', 'm01', 'm12', 'm20', 'c', 'nop'}\n",
        "\n",
        "    @property\n",
        "    def observation_space(self):\n",
        "        return {0, 1}\n",
        "\n",
        "    @property\n",
        "    def temporal_discount_factor(self):\n",
        "        return self.gamma\n",
        "\n",
        "    def state_is_terminal(self, state):\n",
        "        return state == 'T'\n",
        "\n",
        "    def get_reward(self, state, action, next_state=None):\n",
        "        if action == 'nop':\n",
        "            return 0\n",
        "        elif action == 'c':\n",
        "            if state == 0:\n",
        "                return 10\n",
        "            else:\n",
        "                return -100\n",
        "        elif action.startswith('m'):\n",
        "            return -self.c_move\n",
        "        else:  # look\n",
        "            return -self.c_look\n",
        "\n",
        "    def get_transition_distribution(self, state, action):\n",
        "        if action == 'c':\n",
        "            return OnehotDiscreteDistribution('T')\n",
        "        elif action.startswith('m'):\n",
        "            start, target = int(action[1]), int(action[2])\n",
        "            if state == start:\n",
        "                return DiscreteDistribution({target : self.p_move_success, start : 1 - self.p_move_success})\n",
        "        return OnehotDiscreteDistribution(state)\n",
        "\n",
        "    def get_observation_distribution(self, next_state, action):\n",
        "        if action.startswith('l'):\n",
        "            target = int(action[1])\n",
        "            if next_state == target:\n",
        "                return DiscreteDistribution({0: 1 - self.p_obs_if_there, 1: self.p_obs_if_there})\n",
        "            else:\n",
        "                return DiscreteDistribution({0: 1 - self.p_obs_if_not_there, 1: self.p_obs_if_not_there})\n",
        "        return OnehotDiscreteDistribution(0)\n",
        "\n",
        "\n",
        "\n",
        "def bellman_backup(s, V, mdp):\n",
        "    \"\"\"Look ahead one step and propose an update for the value of s.\n",
        "\n",
        "    You can assume that the mdp is either infinite or indefinite\n",
        "    horizon (that is, mdp.horizon is inf).\n",
        "\n",
        "    Args:\n",
        "        s: A state.\n",
        "        V: A dict, V[state] -> value.\n",
        "        mdp: An MDP.\n",
        "\n",
        "    Returns:\n",
        "        vs: new value estimate for s.\n",
        "    \"\"\"\n",
        "\n",
        "    assert mdp.horizon == float(\"inf\")\n",
        "    vs = -float(\"inf\")\n",
        "    for a in mdp.action_space:\n",
        "        qsa = 0.\n",
        "        for ns, p in mdp.get_transition_distribution(s, a).items():\n",
        "            r = mdp.get_reward(s, a, ns)\n",
        "            qsa += p * (r + mdp.temporal_discount_factor * V[ns])\n",
        "        vs = max(qsa, vs)\n",
        "    return vs\n",
        "\n",
        "\n",
        "def value_iteration(mdp, max_num_iters=1000, change_threshold=1e-4):\n",
        "    \"\"\"Run value iteration for a certain number of iterations or until\n",
        "    the max change between iterations is below a threshold.\n",
        "\n",
        "    You can assume that the mdp is either infinite or indefinite\n",
        "    horizon (that is, mdp.horizon is inf).\n",
        "\n",
        "    Args:\n",
        "        mdp: An MDP.\n",
        "        max_num_iters: An int representing the maximum number of\n",
        "        iterations to run value iteration before giving up.\n",
        "        change_threshold: A float used to determine when value iteration\n",
        "        has converged and it is safe to terminate.\n",
        "\n",
        "    Returns:\n",
        "        V:  A dict, V[state] -> value.\n",
        "        it: The number of iterations before convergence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize V to all zeros\n",
        "    V = {s: 0. for s in mdp.state_space}\n",
        "\n",
        "    for it in range(max_num_iters):\n",
        "        next_V = {}\n",
        "        max_change = 0.\n",
        "        for s in mdp.state_space:\n",
        "            if mdp.state_is_terminal(s):\n",
        "                next_V[s] = 0.\n",
        "            else:\n",
        "                next_V[s] = bellman_backup(s, V, mdp)\n",
        "            max_change = max(abs(next_V[s] - V[s]), max_change)\n",
        "        V = next_V\n",
        "        if max_change < change_threshold:\n",
        "            break\n",
        "    return V, it\n",
        "\n",
        "\n",
        "def qsa_from_vs(mdp, V):\n",
        "    \"\"\"Compute Q(s, a) based on V(s).\n",
        "\n",
        "    Args:\n",
        "        mdp: An MDP.\n",
        "        V: A dict, V[state] -> value. Typically, this is computed by value_iteration.\n",
        "\n",
        "    Returns:\n",
        "        Q: A dict, Q[state, action] -> value.\n",
        "    \"\"\"\n",
        "\n",
        "    Q = dict()\n",
        "    for s in mdp.state_space:\n",
        "        if not mdp.state_is_terminal(s):\n",
        "            for a in mdp.action_space:\n",
        "                qsa = 0.\n",
        "                for ns, p in mdp.get_transition_distribution(s, a).items():\n",
        "                    r = mdp.get_reward(s, a, ns)\n",
        "                    qsa += p * (r + mdp.temporal_discount_factor * V[ns])\n",
        "                Q[s, a] = qsa\n",
        "        else:\n",
        "            for a in mdp.action_space:\n",
        "                Q[s, a] = 0\n",
        "    return Q\n",
        "\n",
        "\n",
        "def expectimax_search(initial_state, mdp, horizon, return_Q=False):\n",
        "    \"\"\"Use expectimax search to determine a next action.\n",
        "\n",
        "    Note that we're just computing the single next action to\n",
        "    take, we do not need to store the entire partial V.\n",
        "\n",
        "    Horizon is given as a separate argument so that we can use\n",
        "    expectimax search with receding horizon control, for example,\n",
        "    even if mdp.horizon is inf.\n",
        "\n",
        "    Args:\n",
        "        initial_state: A state in the mdp.\n",
        "        mdp (MDP): An MDP.\n",
        "        horizon (int): An int horizon.\n",
        "        return_Q (bool): A boolean value. If true, also return the Q value\n",
        "            at the root instead of the action.\n",
        "\n",
        "    Returns:\n",
        "        action: An action in the mdp.\n",
        "        Q: The Q value at the root state (only when return_Q is True).\n",
        "    \"\"\"\n",
        "    A = mdp.action_space\n",
        "    R = mdp.get_reward\n",
        "    P = mdp.get_transition_distribution\n",
        "    gm = mdp.temporal_discount_factor\n",
        "    ts = mdp.state_is_terminal\n",
        "\n",
        "    # Cache the V(s, h)'s that have been computed.\n",
        "    def V(s, h):\n",
        "        if h == horizon or ts(s):\n",
        "            return 0\n",
        "        return max(Q(s, a, h) for a in A)\n",
        "\n",
        "    def Q(s, a, h):\n",
        "        psa = P(s, a)\n",
        "        # psa is a DiscreteDistribution over beliefs.  ns is a belief.\n",
        "        return sum(psa.p(ns) * (R(s, a, ns) + gm * V(ns, h+1)) for ns in psa)\n",
        "\n",
        "    Q_values = {a: Q(initial_state, a, 0) for a in A}\n",
        "    if return_Q:\n",
        "        return max(A, key=Q_values.get), Q_values\n",
        "    return max(A, key=Q_values.get)\n",
        "\n",
        "\n",
        "def transition_update(pomdp, belief, action):\n",
        "    \"\"\"Compute p(s') from a prior distribution of p(s) based on the transition\n",
        "    distribution p(s, action, s').\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): A POMDP object.\n",
        "        belief (DiscreteDistribution): A distribution over the current state s.\n",
        "        action: The action to be executed.\n",
        "\n",
        "    Returns:\n",
        "        updated_belief (DiscreteDistribution): A distribution over the next state s'.\n",
        "    \"\"\"\n",
        "    updated = {x: 0.0 for x in pomdp.state_space}\n",
        "    for s in belief:\n",
        "        prob_s = belief.p(s)\n",
        "        for s_prime, transition_prob in pomdp.get_transition_distribution(s, action).items():\n",
        "            updated[s_prime] += transition_prob * prob_s\n",
        "    # Note that we don't necessarily need to renormalize the distribution (it is self-normalized!)\n",
        "    # The added `.renormalize()` part is really just for numeric stability.\n",
        "    return DiscreteDistribution(updated).renormalize()\n",
        "\n",
        "\n",
        "def observation_update(pomdp, belief, action, observation):\n",
        "    \"\"\"Compute p(s' | observation, action) following the Bayes rule.\n",
        "        p(s' | o, a) is proportional to p(s' | a) * p(o | s', a).\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): A POMDP object.\n",
        "        belief (DiscreteDistribution): The distribution over the next state: p(s' | a).\n",
        "            Typically, this is the output of the transition_update() function.\n",
        "        action: The action taken.\n",
        "        observation: The observation.\n",
        "\n",
        "    Returns:\n",
        "        posterior (DiscreteDistribution): The updated belief over the next state s'.\n",
        "            Normalized!\n",
        "    \"\"\"\n",
        "    posterior = {x: 0.0 for x in pomdp.state_space}\n",
        "    for s in belief:\n",
        "        posterior[s] = belief.p(s) * pomdp.get_observation_distribution(s, action).p(observation)\n",
        "    return DiscreteDistribution(posterior).renormalize()\n",
        "\n",
        "\n",
        "def belief_filter(pomdp, belief, action, observation):\n",
        "    \"\"\"Compute the updated belief over the states based on the current action and obervation.\n",
        "\n",
        "    Specifically, the process is:\n",
        "        1. the agent is at state s, and has a belief about its current state p(s).\n",
        "        2. the agent takes an action a, and has a belief about its next state p(s' | a),\n",
        "            computed by transition_update.\n",
        "        3. the agent observes o, which follows the observation model of the POMDP p(o | s', a).\n",
        "        4. the agent updates its belief over the next state p(s' | o, a), following the Bayes rule.\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): A POMDP object.\n",
        "        belief (DiscreteDistribution): The belief about the agent's current state.\n",
        "        action: The action taken.\n",
        "        observation: The observation.\n",
        "\n",
        "    Returns:\n",
        "        next_belief: The belief about the next state by taking into consideration the action\n",
        "            at this step and the observation.\n",
        "    \"\"\"\n",
        "    return observation_update(pomdp, transition_update(pomdp, belief, action),\n",
        "                              action=action, observation=observation)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m4lwbXCky9Y"
      },
      "source": [
        "## Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBKlFwTeky9a"
      },
      "source": [
        "### Belief-Space MDP\n",
        "In this section, you will implement a function `create_belief_mdp`, that transforms a POMDP into a belief-space MDP.\n",
        "    We have provided the basic skeleton for you. In particular, you only need to implement the get_reward and the get_transition_distribution\n",
        "    function for the Belief MDP.  You can use the function `belief_filter(pomdp, b, a, o)` which is already defined; the implementaation of that function is available in the colab.\n",
        "\n",
        "For reference, our solution is **79** lines of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCErY3xmky9b"
      },
      "source": [
        "def create_belief_mdp(pomdp):\n",
        "    \"\"\"Constructs a belief-space MDP from a POMDP.\n",
        "\n",
        "    Args:\n",
        "        pomdp: The input POMDP object.\n",
        "\n",
        "    Returns:\n",
        "        belief_mdp: The constructed belief-space MDP.\n",
        "    \"\"\"\n",
        "    def state_is_terminal(belief):\n",
        "        \"\"\"The state_is_terminal function for the belief-space MDP. It returns true iff. all possible states\n",
        "        in the belief are terminal states.\n",
        "\n",
        "        Args:\n",
        "            belief: A DiscreteDistribution of the state.\n",
        "\n",
        "        Returns:\n",
        "            is_terminal: Whether the current belief is a \"terminal\" belief.\n",
        "        \"\"\"\n",
        "        for state, p in belief.items():\n",
        "            if p > 0 and not pomdp.state_is_terminal(state):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def get_reward(belief, action, next_belief=None):\n",
        "        \"\"\"Compute the expected reward function for the belief-space MDP.\n",
        "\n",
        "        You only need to implement the case where the original reward function only\n",
        "        depends on the state and the action (but not the next state).\n",
        "\n",
        "        In this case, the reward function of the belief-space MDP will be only a function\n",
        "        of belief and action, but not next_belief.\n",
        "\n",
        "        In general (where the reward function if a function of state, action, and next_action),\n",
        "        in order to compute the expected reward, we need to also marginalize over the next state\n",
        "        distribution (which is next_belief).\n",
        "\n",
        "        Args:\n",
        "            belief: A DiscreteDistribution of the state.\n",
        "            action: An action.\n",
        "            next_belief: A DiscreteDistribution of the next state. Should be ignored (see above).\n",
        "\n",
        "        Returns:\n",
        "            reward: the expected reward at this step.\n",
        "        \"\"\"\n",
        "        result = 0\n",
        "        for state, p in belief.items():\n",
        "          result += p*pomdp.get_reward(state,action)\n",
        "        return result\n",
        "\n",
        "    def get_transition_distribution(belief, action):\n",
        "        \"\"\"Compute the transition distribution for an input belief and an action.\n",
        "\n",
        "        Specifically, the output will be a distribution over beliefs. That is, a distribution over\n",
        "        distributions. Since we have restricted our observation space to be finite, the\n",
        "        possible next belief is also a finite space. Thus, we can still use a DiscreteDistribution\n",
        "        object to represent the distribution over the next belief.\n",
        "\n",
        "        Args:\n",
        "            belief: A DiscreteDistribution of the state.\n",
        "            action: An action.\n",
        "\n",
        "        Returns:\n",
        "            next_belief: A DiscreteDistribution of the next state.\n",
        "        \"\"\"\n",
        "        observation_prob = {o: 0 for o in pomdp.observation_space}\n",
        "        for state, state_p in belief.items():\n",
        "            next_state_distribution = pomdp.get_transition_distribution(state, action)\n",
        "            for next_state, next_state_p in next_state_distribution.items():\n",
        "                next_observation_distribution = pomdp.get_observation_distribution(next_state, action)\n",
        "                for obs, obs_p in next_observation_distribution.items():\n",
        "                    observation_prob[obs] += state_p * next_state_p * obs_p\n",
        "        next_belief_prob = dict()\n",
        "        for obs, obs_p in observation_prob.items():\n",
        "            if obs_p > 0:\n",
        "                next_belief = belief_filter(pomdp, belief, action, obs)\n",
        "                next_belief_prob[next_belief] = obs_p\n",
        "        return DiscreteDistribution(next_belief_prob)\n",
        "        # result = {}\n",
        "        # for ob in pomdp.observation_space:\n",
        "        #   temp = {}\n",
        "        #   total = 0\n",
        "        #   for st1 in pomdp.state_space:\n",
        "        #     temp[st1] = 0\n",
        "        #     for st0, p in belief.items():\n",
        "        #       temp[st1] += p * pomdp.get_transition_distribution(st0,action).p(st1)\n",
        "        #       #total+= pomdp.get_transition_distribution(st0,action).p(st1) * pomdp.get_observation_distribution(st1, action).p(ob)\n",
        "        #     #print(st,ob,pomdp.get_observation_distribution(st, action).p(ob))\n",
        "        #     temp[st1] *= pomdp.get_observation_distribution(st1, action).p(ob)\n",
        "        #     total+= temp[st1]\n",
        "        #   if sum(temp.values()) > 0:\n",
        "        #     result[DiscreteDistribution(temp).renormalize()] = total\n",
        "        # return DiscreteDistribution(result).renormalize()\n",
        "\n",
        "    # Construct a new MDP based on the functions defined above.\n",
        "    return LambdaMDP(\n",
        "        state_space=None,  # We are not going to specify the state space explicitly (it's a continuous space).\n",
        "        action_space=pomdp.action_space,\n",
        "        state_is_terminal_fn=state_is_terminal,\n",
        "        get_reward_fn=get_reward,\n",
        "        get_transition_distribution_fn=get_transition_distribution,\n",
        "        temporal_discount_factor=pomdp.temporal_discount_factor\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWxIpv_6ky9b"
      },
      "source": [
        "Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwz8QF4Yky9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82e4b47-99f5-4c2d-bbb2-d04945176b7f"
      },
      "source": [
        "def test1_create_belief_mdp():\n",
        "    pomdp = RobotChargingPOMDP()\n",
        "    belief_mdp = create_belief_mdp(pomdp)\n",
        "    b4 = DiscreteDistribution({0 : .4, 1: .3, 2: .3})\n",
        "    a, Q = expectimax_search(b4, belief_mdp, 4, return_Q=True)\n",
        "    print(Q)\n",
        "\n",
        "    assert a == 'l1'\n",
        "    gt = {'l0': -0.1, 'l1': 0.255914, 'l2': -0.1, 'm12': -0.471128, 'm01': -0.5, 'm20': -0.031586, 'c': -56.0, 'nop': 0.0}\n",
        "    for k, v in gt.items():\n",
        "        assert k in Q and np.allclose(v, Q[k])\n",
        "\n",
        "test1_create_belief_mdp()\n",
        "print('Tests passed.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'l1': 0.2559140000000006, 'l0': -0.1, 'm20': -0.03158599999999905, 'm01': -0.5, 'm12': -0.4711279999999987, 'l2': -0.1, 'nop': 0.0, 'c': -56.0}\n",
            "Tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6PJ1JzRky9e"
      },
      "source": [
        "### Receding Horizon Control\n",
        "Use the RHC implementation provided in the colab and answer the questions in Catsoop.  **Note that the simulation function sets the random seed so that your results will be deterministic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LugPJbeKky9e"
      },
      "source": [
        "A simple implementation of the Receding Horizon Control (RHC).\n",
        "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj90tA-vky9f"
      },
      "source": [
        "def receding_horizon_control(pomdp, h=4, search_algo=expectimax_search):\n",
        "    \"\"\"Receding Horizon Control (RHC).\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): The input POMDP problem.\n",
        "        h (int): The receding horizon.\n",
        "\n",
        "    Returns:\n",
        "        policy (Callable): A function policy(belief) -> action, mapping from a belief state to the action to take.\n",
        "    \"\"\"\n",
        "\n",
        "    belief_mdp = create_belief_mdp(pomdp)\n",
        "\n",
        "    def policy(belief):\n",
        "        \"\"\"The RHC policy. Basically, it runs a search algorithm (e.g., expectimax_search)\n",
        "            with a fixed horizon and output the optimal policy at the current belief.\n",
        "\n",
        "        Args:\n",
        "            belief (DiscreteDistribution): The current belief.\n",
        "\n",
        "        Returns:\n",
        "            action: The next action to take.\n",
        "        \"\"\"\n",
        "\n",
        "        return search_algo(belief, belief_mdp, horizon=h)\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "def simulate(pomdp, initial_belief, policy, n=4, real_s=None):\n",
        "    \"\"\"Simulate a policy on a POMDP.\n",
        "\n",
        "    Specifically, the function commands the environment for n timesteps.\n",
        "    We will keep track of two variables.\n",
        "        - robot_b, which is the current belief.\n",
        "        - real_s: the \"true\" state of the world.\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): The input POMDP problem.\n",
        "        initial_belief (DiscreteDistribution): a distribution of the state (the initial belief).\n",
        "        n (int): The nunber of simulation steps.\n",
        "        real_s: The initial real_s, can be None, in which case it will be sampled from initial_belief.\n",
        "        policy_gen (Callable): a function that maps a belief to the next action.\n",
        "    \"\"\"\n",
        "\n",
        "    import numpy.random as npr; npr.seed(0)  # to determinize the execution.\n",
        "\n",
        "    # Create the Belief MDP.\n",
        "    bmdp = create_belief_mdp(pomdp)\n",
        "\n",
        "    robot_b = initial_belief\n",
        "    if real_s is None:\n",
        "        real_s = robot_b.draw()\n",
        "\n",
        "    print('Robot belief:', robot_b)\n",
        "    print('Real state:', real_s)\n",
        "    print('')\n",
        "\n",
        "    for t in range(n):\n",
        "        print('Step', t)\n",
        "        # search_algo returns the optimal action at the current belief.\n",
        "        a = policy(robot_b)\n",
        "\n",
        "        print('  Executing:', a)\n",
        "        real_s = pomdp.get_transition_distribution(real_s, a).draw()\n",
        "        print('  Real State:', real_s)\n",
        "        if pomdp.state_is_terminal(real_s):\n",
        "            print('Terminated.')\n",
        "            break\n",
        "        o = pomdp.get_observation_distribution(real_s, a).draw()\n",
        "        print('  Observation:', o)\n",
        "        robot_b = belief_filter(pomdp, robot_b, a, o)\n",
        "        print('  Robot belief:', robot_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u149gKqky9g"
      },
      "source": [
        "You can run the following code to visualize the execution of your RHC-Expectimax policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn6jbQS0ky9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc0b604-0204-4734-88ed-3899c9e65286"
      },
      "source": [
        "pomdp = RobotChargingPOMDP(gamma=0.9)\n",
        "b0 = DiscreteDistribution({0: 0.03, 1: 0.07, 2: 0.9})\n",
        "policy = receding_horizon_control(pomdp, 3, search_algo=expectimax_search)\n",
        "simulate(pomdp, b0, policy=policy, n=10, real_s=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robot belief: {0: 0.03, 1: 0.07, 2: 0.9}\n",
            "Real state: 2\n",
            "\n",
            "Step 0\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.03, 1: 0.07, 2: 0.9, 'T': 0.0}\n",
            "Step 1\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.03, 1: 0.07, 2: 0.9, 'T': 0.0}\n",
            "Step 2\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.03, 1: 0.07, 2: 0.9, 'T': 0.0}\n",
            "Step 3\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.03, 1: 0.07, 2: 0.9, 'T': 0.0}\n",
            "Step 4\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.03, 1: 0.07, 2: 0.9, 'T': 0.0}\n",
            "Step 5\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.03, 1: 0.07, 2: 0.9, 'T': 0.0}\n",
            "Step 6\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.03, 1: 0.07, 2: 0.9, 'T': 0.0}\n",
            "Step 7\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.03, 1: 0.07, 2: 0.9, 'T': 0.0}\n",
            "Step 8\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.03, 1: 0.07, 2: 0.9, 'T': 0.0}\n",
            "Step 9\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.03, 1: 0.07, 2: 0.9, 'T': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts-NVcqPky9g"
      },
      "source": [
        "### Receding Horizon Control with Most-Likely-Observation\n",
        "Complete the RHC-MLO implementation provided in the colab and answer the questions in Catsoop.\n",
        "The only new requirement is to implement the `expectimax_search_mlo` that instead of computing the\n",
        "expected reward over all possible observations, only focuses on the most-likely observation.<br />\n",
        "**HINT**: You only need very MINIMAL changes to the original `expectimax_search` code.<br />\n",
        "**HINT**: You may find the `DiscreteDistribution.max()` function useful.\n",
        "\n",
        "After implementing `expectimax_search_mlo`, use the following code snippets (in colab) to simulate RHC-MLO, and answer questions in Catsoop.\n",
        "Specifically, we will use `gamma = 0.9` and initial belief $b = (0.4, 0.3, 0.3)$ (the same one we used in the Most Likely State problem).  Note that we are specifying the actual initial state to be 2.\n",
        "\n",
        "\n",
        "For reference, our solution is **20** lines of code.\n",
        "\n",
        "In addition to all of the utilities defined at the top of the colab notebook, the following functions are available in this question environment: `create_belief_mdp`. You may not need to use all of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC_pOzo8ky9h"
      },
      "source": [
        "def expectimax_search_mlo(initial_state, belief_mdp, horizon, return_Q=False):\n",
        "    \"\"\"Use expectimax search to determine a next action.\n",
        "\n",
        "    Note that we're just computing the single next action to\n",
        "    take, we do not need to store the entire partial V.\n",
        "\n",
        "    Horizon is given as a separate argument so that we can use\n",
        "    expectimax search with receding horizon control, for example,\n",
        "    even if belief_mdp.horizon is inf.\n",
        "\n",
        "    Args:\n",
        "        initial_state: A state in the belief_mdp.\n",
        "        belief_mdp: An MDP.\n",
        "        horizon: An int horizon.\n",
        "        return_Q: A boolean value. If true, also return the Q value\n",
        "            at the root instead of the action.\n",
        "\n",
        "    Returns:\n",
        "        action: An action in the belief_mdp.\n",
        "        Q: The Q value at the root state (only when return_Q is True).\n",
        "    \"\"\"\n",
        "    A = belief_mdp.action_space\n",
        "    R = belief_mdp.get_reward\n",
        "    P = belief_mdp.get_transition_distribution\n",
        "    gm = belief_mdp.temporal_discount_factor\n",
        "    ts = belief_mdp.state_is_terminal\n",
        "\n",
        "    def V(s, h):\n",
        "        if h == horizon or ts(s):\n",
        "            return 0\n",
        "        return max(Q(s, a, h) for a in A)\n",
        "\n",
        "    def Q(s, a, h):\n",
        "        psa = P(s, a)\n",
        "        maxns = psa.max()\n",
        "        # psa is a DiscreteDistribution over beliefs.  ns is a belief.\n",
        "        return (R(s, a, maxns) + gm * V(maxns, h+1))\n",
        "\n",
        "\n",
        "    Q_values = {a: Q(initial_state, a, 0) for a in A}\n",
        "    if return_Q:\n",
        "        return max(A, key=Q_values.get), Q_values\n",
        "    return max(A, key=Q_values.get)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr1Zw9jAky9h"
      },
      "source": [
        "Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ntWcK6eky9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "392e412f-63aa-42d9-95fe-a6a2a41c4f0a"
      },
      "source": [
        "def test1_mlo():\n",
        "    pomdp = RobotChargingPOMDP(gamma=0.9)\n",
        "    policy = receding_horizon_control(pomdp, 4, search_algo=expectimax_search_mlo)\n",
        "\n",
        "    b0 = DiscreteDistribution({0: 0.4, 1: 0.3, 2: 0.3})\n",
        "    assert policy(b0) == 'm20'\n",
        "    b1 = DiscreteDistribution({0: 0.64, 1: 0.3, 2: 0.05999999999999998, 'T': 0.0})\n",
        "    assert policy(b1) == 'l0'\n",
        "    b2 = DiscreteDistribution({0: 0.8, 1: 0.16666666666666666, 2: 0.03333333333333331})\n",
        "    assert policy(b2) == 'l1'\n",
        "\n",
        "test1_mlo()\n",
        "print('Tests passed.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCvrTY1dky9i"
      },
      "source": [
        "You can run the following code to visualize the execution of your RHC-Expectimax-MLO policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLeai_fAky9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec63d54-78d9-4e2c-b068-e27aba990d58"
      },
      "source": [
        "pomdp = RobotChargingPOMDP(gamma=0.9)\n",
        "b0 = DiscreteDistribution({0: 0.4, 1: 0.3, 2: 0.3})\n",
        "policy = receding_horizon_control(pomdp, 4, search_algo=expectimax_search_mlo)\n",
        "simulate(pomdp, b0, policy=policy, n=10, real_s=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robot belief: {0: 0.4, 1: 0.3, 2: 0.3}\n",
            "Real state: 2\n",
            "\n",
            "Step 0\n",
            "  Executing: m20\n",
            "  Real State: 0\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.64, 1: 0.3, 2: 0.05999999999999998, 'T': 0.0}\n",
            "Step 1\n",
            "  Executing: l0\n",
            "  Real State: 0\n",
            "  Observation: 1\n",
            "  Robot belief: {0: 0.8, 1: 0.16666666666666666, 2: 0.03333333333333331, 'T': 0.0}\n",
            "Step 2\n",
            "  Executing: l1\n",
            "  Real State: 0\n",
            "  Observation: 1\n",
            "  Robot belief: {0: 0.6620689655172415, 1: 0.31034482758620685, 2: 0.027586206896551703, 'T': 0.0}\n",
            "Step 3\n",
            "  Executing: l0\n",
            "  Real State: 0\n",
            "  Observation: 1\n",
            "  Robot belief: {0: 0.8150943396226414, 1: 0.16981132075471694, 2: 0.015094339622641496, 'T': 0.0}\n",
            "Step 4\n",
            "  Executing: l1\n",
            "  Real State: 0\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.9494505494505494, 1: 0.03296703296703296, 2: 0.017582417582417565, 'T': 0.0}\n",
            "Step 5\n",
            "  Executing: l1\n",
            "  Real State: 0\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.976271186440678, 1: 0.005649717514124293, 2: 0.018079096045197723, 'T': 0.0}\n",
            "Step 6\n",
            "  Executing: l2\n",
            "  Real State: 0\n",
            "  Observation: 1\n",
            "  Robot belief: {0: 0.9546961325966852, 1: 0.005524861878453038, 2: 0.03977900552486184, 'T': 0.0}\n",
            "Step 7\n",
            "  Executing: l2\n",
            "  Real State: 0\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.9874285714285715, 1: 0.005714285714285713, 2: 0.006857142857142849, 'T': 0.0}\n",
            "Step 8\n",
            "  Executing: c\n",
            "  Real State: T\n",
            "Terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPkEpr7uky9j"
      },
      "source": [
        "### QMDP\n",
        "Complete the QMDP implementation provided in the colab and answer the questions in catsoop. Here you can use the value function\n",
        "computed by the `value_iteration` algorithm provided and focus on computing the policy.\n",
        "\n",
        "For reference, our solution is **20** lines of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drXs2a-Iky9k"
      },
      "source": [
        "def qmdp(pomdp):\n",
        "    \"\"\"QMDP algorithm.\n",
        "    This function takes a POMDP as input and output a policy function that maps belief to the action to take.\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): An POMDP.\n",
        "\n",
        "    Returns:\n",
        "        policy (Callable): A function policy(belief) -> action, mapping from a belief state to the action to take.\n",
        "    \"\"\"\n",
        "    V, _ = value_iteration(pomdp)  # run value iteration on the underlying MDP.\n",
        "    Q = qsa_from_vs(pomdp, V)   # constructs Q values from the value function.\n",
        "\n",
        "    def policy(belief):\n",
        "        \"\"\"The QMDP policy.\n",
        "\n",
        "        Args:\n",
        "            belief (DiscreteDistribution): The belief about the current state.\n",
        "\n",
        "        Returns:\n",
        "            action: The argmax action computed based on QMDP.\n",
        "        \"\"\"\n",
        "        maxvalue = -1000000\n",
        "        maxaction = None\n",
        "        for a in pomdp.action_space:\n",
        "          temp = 0\n",
        "          for state in pomdp.state_space:\n",
        "            temp+= belief.p(state)*Q[state,a]\n",
        "          if temp > maxvalue:\n",
        "            maxvalue = temp\n",
        "            maxaction = a\n",
        "        return maxaction\n",
        "    return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJNVb8yyky9k"
      },
      "source": [
        "Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efXkD3NPky9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c442bd8-a62c-49d5-e10c-837a247257c9"
      },
      "source": [
        "def test1_qmdp():\n",
        "    pomdp = RobotChargingPOMDP()\n",
        "    policy = qmdp(pomdp)\n",
        "    b0 = DiscreteDistribution({0: 0.999, 1: 0.001, 2: 0.0})\n",
        "    assert policy(b0) == 'c'\n",
        "    b0 = DiscreteDistribution({0: 0.0, 1: 0.6, 2: 0.4})\n",
        "    assert policy(b0) == 'm12'\n",
        "    b0 = DiscreteDistribution({0: 0.1, 1: 0.33, 2: 0.57})\n",
        "    assert policy(b0) == 'm20'\n",
        "\n",
        "test1_qmdp()\n",
        "print('Tests passed.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxSaDQeqky9l"
      },
      "source": [
        "You can run the following code to visualize the execution of your QMDP policy (Hint: QMDP is not a good strategy for this case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOGYs9EVky9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f4e34a-1798-4acb-8df8-40e43e2292c3"
      },
      "source": [
        "pomdp = RobotChargingPOMDP(gamma=0.9)\n",
        "b0 = DiscreteDistribution({0: 0.5, 1: 0.3, 2: 0.2})\n",
        "policy = qmdp(pomdp)\n",
        "simulate(pomdp, b0, policy=policy, n=10, real_s=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robot belief: {0: 0.5, 1: 0.3, 2: 0.2}\n",
            "Real state: 2\n",
            "\n",
            "Step 0\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.5, 1: 0.3, 2: 0.2, 'T': 0.0}\n",
            "Step 1\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.5, 1: 0.3, 2: 0.2, 'T': 0.0}\n",
            "Step 2\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.5, 1: 0.3, 2: 0.2, 'T': 0.0}\n",
            "Step 3\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.5, 1: 0.3, 2: 0.2, 'T': 0.0}\n",
            "Step 4\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.5, 1: 0.3, 2: 0.2, 'T': 0.0}\n",
            "Step 5\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.5, 1: 0.3, 2: 0.2, 'T': 0.0}\n",
            "Step 6\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.5, 1: 0.3, 2: 0.2, 'T': 0.0}\n",
            "Step 7\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.5, 1: 0.3, 2: 0.2, 'T': 0.0}\n",
            "Step 8\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.5, 1: 0.3, 2: 0.2, 'T': 0.0}\n",
            "Step 9\n",
            "  Executing: nop\n",
            "  Real State: 2\n",
            "  Observation: 0\n",
            "  Robot belief: {0: 0.5, 1: 0.3, 2: 0.2, 'T': 0.0}\n"
          ]
        }
      ]
    }
  ]
}