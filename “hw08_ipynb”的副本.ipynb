{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "“hw08.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa2a04e7"
      },
      "source": [
        "# Homework 8"
      ],
      "id": "aa2a04e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c82d51b"
      },
      "source": [
        "## Imports and Utilities\n",
        "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
      ],
      "id": "9c82d51b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68f6f09a"
      },
      "source": [
        "from collections import defaultdict\n",
        "from math import sqrt, log\n",
        "import abc\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MDP:\n",
        "    \"\"\"A Markov Decision Process.\"\"\"\n",
        "\n",
        "    @property\n",
        "    @abc.abstractmethod\n",
        "    def state_space(self):\n",
        "        \"\"\"Representation of the MDP state set.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @property\n",
        "    @abc.abstractmethod\n",
        "    def action_space(self):\n",
        "        \"\"\"Representation of the MDP action set.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @property\n",
        "    def temporal_discount_factor(self):\n",
        "        \"\"\"Gamma, defaults to 1.\n",
        "        \"\"\"\n",
        "        return 1.\n",
        "\n",
        "    @property\n",
        "    def horizon(self):\n",
        "        \"\"\"H, defaults to inf.\n",
        "        \"\"\"\n",
        "        return float(\"inf\")\n",
        "\n",
        "    def state_is_terminal(self, state):\n",
        "        \"\"\"Designate certain states as terminal (done) states.\n",
        "\n",
        "        Defaults to False.\n",
        "\n",
        "        Args:\n",
        "            state: A state.\n",
        "\n",
        "        Returns:\n",
        "            state_is_terminal : A bool.\n",
        "        \"\"\"\n",
        "        return False\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_reward(self, state, action, next_state):\n",
        "        \"\"\"Return (deterministic) reward for executing action\n",
        "        in state.\n",
        "\n",
        "        Args:\n",
        "            state: A current state.\n",
        "            action: An action.\n",
        "            next_state: A next state.\n",
        "\n",
        "        Returns:\n",
        "            reward : Single time step reward.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_transition_distribution(self, state, action):\n",
        "        \"\"\"Return a distribution over next states.\n",
        "\n",
        "        The form of this distribution will vary, e.g., depending\n",
        "        on whether the MDP has discrete or continuous states.\n",
        "\n",
        "        Args:\n",
        "            state: A current state.\n",
        "            action: An action.\n",
        "\n",
        "        Returns:\n",
        "            next_state_distribution: Distribution over next states.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    def sample_next_state(self, state, action, rng=np.random):\n",
        "        \"\"\"Sample a next state from the transition distribution.\n",
        "\n",
        "        This function may be overwritten by subclasses when the explicit\n",
        "        distribution is too large to enumerate.\n",
        "\n",
        "        Args:\n",
        "            state: A state from the state space.\n",
        "            action: An action from the action space.\n",
        "            rng: A random number generator.\n",
        "\n",
        "        Returns:\n",
        "            next_state: A sampled next state from the state space.\n",
        "        \"\"\"\n",
        "        next_state_dist = self.get_transition_distribution(state, action)\n",
        "        next_states, probs = zip(*next_state_dist.items())\n",
        "        next_state_index = rng.choice(len(next_states), p=probs)\n",
        "        next_state = next_states[next_state_index]\n",
        "        return next_state\n",
        "\n",
        "\n",
        "class SingleRowMDP(MDP):\n",
        "    \"\"\"A 1D grid MDP for debugging. The grid is 1x5\n",
        "    and the agent is meant to start off in the middle.\n",
        "    There is +10 reward on the rightmost square, -10 on\n",
        "    the left. Actions are left and right. An action effect\n",
        "    is reversed with 10% probability.\n",
        "    \"\"\"\n",
        "    @property\n",
        "    def state_space(self):\n",
        "        return {0, 1, 2, 3, 4}  # position in grid\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return {0, 1}  # left, right\n",
        "\n",
        "    def get_transition_distribution(self, state, action):\n",
        "        # Discrete distributions, represented with a dict\n",
        "        # mapping next states to probs.\n",
        "        delta = 1 if action == 1 else -1\n",
        "        intended_effect = min(max(state + delta, 0), 4)\n",
        "        opposite_effect = min(max(state - delta, 0), 4)\n",
        "        assert (intended_effect != opposite_effect)\n",
        "        return {intended_effect: 0.9, opposite_effect: 0.1}\n",
        "\n",
        "    def get_reward(self, state, action, next_state):\n",
        "        if next_state == 0:\n",
        "          return -10\n",
        "        if next_state == 4:\n",
        "          return 10\n",
        "        return -1  # living penalty\n",
        "\n",
        "    def state_is_terminal(self, state):\n",
        "        return state in {0, 4}\n",
        "\n",
        "\n",
        "class MarshmallowMDP(MDP):\n",
        "    \"\"\"The Marshmallow MDP described in lecture.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def state_space(self):\n",
        "        # (hunger level, marshmallow remains)\n",
        "        return {(h, m) for h in {0, 1, 2} for m in {True, False}}\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return {\"eat\", \"wait\"}\n",
        "\n",
        "    @property\n",
        "    def horizon(self):\n",
        "        return 4\n",
        "\n",
        "    def get_reward(self, state, action, next_state):\n",
        "        next_hunger_level = next_state[0]\n",
        "        return -(next_hunger_level**2)\n",
        "\n",
        "    def get_transition_distribution(self, state, action):\n",
        "        # Update marshmallow deterministically\n",
        "        if action == \"eat\":\n",
        "            next_m = False\n",
        "        else:\n",
        "            next_m = state[1]\n",
        "\n",
        "        # Initialize next state distribution dict\n",
        "        # Any state not included assumed to have 0 prob\n",
        "        dist = defaultdict(float)\n",
        "\n",
        "        # Update hunger\n",
        "        if action == \"wait\" or state[1] == False:\n",
        "            # With 0.75 probability, hunger stays the same\n",
        "            dist[(state[0], next_m)] += 0.75\n",
        "            # With 0.25 probability, hunger increases by 1\n",
        "            dist[(min(state[0] + 1, 2), next_m)] += 0.25\n",
        "\n",
        "        else:\n",
        "            assert action == \"eat\" and state[1] == True\n",
        "            # Hunger deterministically set to 1 after eating\n",
        "            dist[(0, next_m)] = 1.0\n",
        "\n",
        "        return dist\n",
        "\n",
        "\n",
        "class ZitsMDP(MDP):\n",
        "    \"\"\"The Zits MDP described in lecture.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def state_space(self):\n",
        "        return {0, 1, 2, 3, 4}\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return {\"apply\", \"sleep\"}\n",
        "\n",
        "    @property\n",
        "    def temporal_discount_factor(self):\n",
        "        return 0.9\n",
        "\n",
        "    def get_reward(self, state, action, next_state):\n",
        "        if action == \"apply\":\n",
        "            return -1 - next_state\n",
        "        assert action == \"sleep\"\n",
        "        return -next_state\n",
        "\n",
        "    def get_transition_distribution(self, state, action):\n",
        "        if action == \"apply\":\n",
        "            return {\n",
        "                0: 0.8,\n",
        "                4: 0.2\n",
        "            }\n",
        "        assert action == \"sleep\"\n",
        "        return {\n",
        "            min(state + 1, 4): 0.4,\n",
        "            max(state - 1, 0): 0.6\n",
        "        }\n",
        "\n",
        "\n",
        "class ChaseMDP(MDP):\n",
        "    \"\"\"A 2D grid bunny chasing MDP.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def obstacles(self):\n",
        "        return np.zeros((2, 3))  # by default, 2x3 grid with no obstacles\n",
        "\n",
        "    @property\n",
        "    def goal_reward(self):\n",
        "        return 1\n",
        "\n",
        "    @property\n",
        "    def living_reward(self):\n",
        "        return 0\n",
        "\n",
        "    @property\n",
        "    def height(self):\n",
        "        return self.obstacles.shape[0]\n",
        "\n",
        "    @property\n",
        "    def width(self):\n",
        "        return self.obstacles.shape[1]\n",
        "\n",
        "    @property\n",
        "    def state_space(self):\n",
        "        pos = [(r, c) for r in range(self.height) for c in range(self.width)]\n",
        "        return {(p1, p2) for p1 in pos for p2 in pos}\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return {'up', 'down', 'left', 'right'}\n",
        "\n",
        "    @property\n",
        "    def temporal_discount_factor(self):\n",
        "        return 0.9\n",
        "\n",
        "    def action_to_delta(self, action):\n",
        "        return {\n",
        "            'up': (-1, 0),  # up,\n",
        "            'down': (1, 0),  # down,\n",
        "            'left': (0, -1),  # left,\n",
        "            'right': (0, 1),  # right,\n",
        "        }[action]\n",
        "\n",
        "    def get_transition_distribution(self, state, action):\n",
        "        # Discrete distributions, represented with a dict\n",
        "        # mapping next states to probs.\n",
        "        next_state_dist = defaultdict(float)\n",
        "\n",
        "        agent_pos, goal_pos = state\n",
        "\n",
        "        # Get next agent state\n",
        "        row, col = agent_pos\n",
        "        dr, dc = self.action_to_delta(action)\n",
        "        r, c = row + dr, col + dc\n",
        "        # Stay in place if out of bounds or obstacle\n",
        "        if not (0 <= r < self.height and 0 <= c < self.width):\n",
        "            r, c = row, col\n",
        "        elif self.obstacles[r, c]:\n",
        "            r, c = row, col\n",
        "        next_agent_pos = (r, c)\n",
        "\n",
        "        # Get next bunny state\n",
        "        # Stay in same place with probability 0.5\n",
        "        next_state_dist[(next_agent_pos, goal_pos)] += 0.5\n",
        "        # Otherwise move\n",
        "        row, col = goal_pos\n",
        "        for (dr, dc) in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
        "            r, c = row + dr, col + dc\n",
        "            # Stay in place if out of bounds or obstacle\n",
        "            if not (0 <= r < self.height and 0 <= c < self.width):\n",
        "                r, c = row, col\n",
        "            elif self.obstacles[r, c]:\n",
        "                r, c = row, col\n",
        "            next_goal_pos = (r, c)\n",
        "            next_state_dist[(next_agent_pos, next_goal_pos)] += 0.5*0.25\n",
        "\n",
        "        return next_state_dist\n",
        "\n",
        "    def get_reward(self, state, action, next_state):\n",
        "        agent_pos, goal_pos = next_state\n",
        "        if agent_pos == goal_pos:\n",
        "            return self.goal_reward\n",
        "        return self.living_reward\n",
        "\n",
        "    def state_is_terminal(self, state):\n",
        "        agent_pos, goal_pos = state\n",
        "        return agent_pos == goal_pos\n",
        "\n",
        "\n",
        "class LargeChaseMDP(ChaseMDP):\n",
        "    \"\"\"A larger 2D grid bunny chasing MDP.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def obstacles(self):\n",
        "        return np.array([\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
        "            [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
        "            [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
        "            [0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "        ])\n",
        "\n"
      ],
      "id": "68f6f09a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb914877"
      },
      "source": [
        "## Problems"
      ],
      "id": "cb914877"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df618423"
      },
      "source": [
        "### Stochastic Rollout\n",
        "Complete the implementation of a stochastic rollout function.\n",
        "\n",
        "For reference, our solution is **15** lines of code."
      ],
      "id": "df618423"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9769868f"
      },
      "source": [
        "def stochastic_rollout(mdp, state, nr_rollouts=10, max_depth=10):\n",
        "  \"\"\"Stochastic rollout for estimating the value of a given state.\n",
        "\n",
        "  We will use the simplest rollout policy: at each step, we uniformly sample\n",
        "  a random action from the action space.\n",
        "\n",
        "  Your implementation should rollout for `nr_rollouts` times, and take the average\n",
        "  value to reduce variance.\n",
        "\n",
        "  Your code should also take care of the maximum rollout steps to avoid infinite\n",
        "  looping. That is, the maximum depth of the search tree.\n",
        "  A typical way to handle this is to return 0 during your rollout function\n",
        "  when the maximum depth is reached.\n",
        "\n",
        "  IMPORTANT: Please use the following code snippets to get the list of actions\n",
        "  when you do the sampling, to ensure the algorithm runs deterministically.\n",
        "  `actions = sorted(mdp.action_space)`\n",
        "\n",
        "  Args:\n",
        "      mdp: an MDP.\n",
        "      state: the state.\n",
        "      nr_rollouts: an integer, indicating the number of rollouts.\n",
        "      max_depth: the max number of actions for the rollout.\n",
        "\n",
        "  Return:\n",
        "      v: the average value at this state.\n",
        "  \"\"\"\n",
        "  actions = sorted(list(mdp.action_space))\n",
        "  def dfs(s, depth):\n",
        "      if depth == 0:\n",
        "          return 0\n",
        "      if mdp.state_is_terminal(s):\n",
        "          return 0\n",
        "      a = actions[np.random.choice(len(actions))]\n",
        "      ns = mdp.sample_next_state(s, a)\n",
        "      reward = mdp.get_reward(s, a, ns)\n",
        "      return reward + mdp.temporal_discount_factor * dfs(ns, depth-1)\n",
        "\n",
        "  average = 0\n",
        "  for i in range(nr_rollouts):\n",
        "      average += dfs(state, max_depth)\n",
        "  return average / nr_rollouts"
      ],
      "id": "9769868f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c15babf4"
      },
      "source": [
        "Tests"
      ],
      "id": "c15babf4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6123525f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c2cec0-d543-463e-d6cc-2f128c218ee5"
      },
      "source": [
        "def test1_uct_rollout():\n",
        "    mdp = SingleRowMDP()\n",
        "    import random; random.seed(0)\n",
        "    import numpy.random as npr; npr.seed(0)\n",
        "    assert stochastic_rollout(mdp, 2) == -6.1\n",
        "\n",
        "test1_uct_rollout()\n",
        "def test2_uct_rollout():\n",
        "    mdp = ZitsMDP()\n",
        "    import random; random.seed(0)\n",
        "    import numpy.random as npr; npr.seed(0)\n",
        "    rets = [stochastic_rollout(mdp, i, 1, 3) for i in range(5)]\n",
        "    gt = [-1.9, -4.95, -1.81, -10.75, -9.55]\n",
        "    assert np.allclose(rets, gt, atol=1e-5)\n",
        "\n",
        "test2_uct_rollout()\n",
        "print('Tests passed.')"
      ],
      "id": "6123525f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d8eeae8"
      },
      "source": [
        "### UCT Exploration\n",
        "Complete the implementation of the UCT exploration policy.\n",
        "\n",
        "For reference, our solution is **12** lines of code."
      ],
      "id": "5d8eeae8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f68a5616"
      },
      "source": [
        "def explore(mdp, depth, s, Q, N, exploration_factor=1.0):\n",
        "  \"\"\"Compute the UCT policy at depth `depth` and state `s`, based on the current estimation of Q and N.\n",
        "  You can assume that Q and N are both dictionaries mapping tuples (depth, s, a) into the corresponding\n",
        "  number of visits N[depth, s, a] and Q values Q[depth, s, a].\n",
        "\n",
        "  The term `exploration_factor` is used to balance the Q value of a state-action\n",
        "  pair and the UCB term for that pair.\n",
        "\n",
        "      UCB(depth, s, a) = Q(depth, s, a) + exploration_factor * sqrt(log(sn)/ n)\n",
        "\n",
        "  where n = N(depth, s, a) and sn = sum_a N(depth, s, a).\n",
        "  Remember to handle n = 0, in which case the corresponding UCB should be inf.\n",
        "\n",
        "  In this case, you can rely on the tie breaking behavior of the \"max\" action in Python.\n",
        "  Specifically, if there are multiple values with score np.inf, select the last action based\n",
        "  on the order of sorted(list(mdp.action_space)).\n",
        "\n",
        "  Args:\n",
        "      mdp: an MDP.\n",
        "      depth: current depth of the search.\n",
        "      s: the current state.\n",
        "      Q: a dictionary mapping (depth, s, a) to the corresponding Q value.\n",
        "      N: a dictionary mapping (depth, s, a) to the corresponding number of visits.\n",
        "      exploration_factor: a floating-point number. It is the scalar hyperparameter\n",
        "          applied to the UCB term when choosing which action to expand.\n",
        "          In the lecture notes, this is refered to as c.\n",
        "\n",
        "  Return:\n",
        "      a: the action to be taken at state s. This is max_a UCB(depth, s, a).\n",
        "  \"\"\"\n",
        "  actions = sorted(list(mdp.action_space))\n",
        "  n = sum(N[depth, s, a] for a in actions) or 1\n",
        "  ucb = [(\n",
        "        Q[depth, s, a] + (\n",
        "            exploration_factor * sqrt(log(n) / N[depth, s, a])\n",
        "            if n > 0 and N[depth, s, a] > 0\n",
        "            else np.inf\n",
        "        ),\n",
        "        a\n",
        "  ) for a in actions]\n",
        "  return max(ucb)[1]"
      ],
      "id": "f68a5616",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7a1a9c4"
      },
      "source": [
        "Tests"
      ],
      "id": "a7a1a9c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baec7367",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ad0c5b-36c2-4d45-ead7-ce63c753131a"
      },
      "source": [
        "def test1_uct_explore():\n",
        "    mdp = SingleRowMDP()\n",
        "    Q = {(0, 3, 0): 0.5, (0, 3, 1): 0.3}\n",
        "    N = {(0, 3, 0): 5, (0, 3, 1): 1}\n",
        "    assert explore(mdp, 0, 3, Q, N, 1.0) == 1\n",
        "\n",
        "test1_uct_explore()\n",
        "def test2_uct_explore():\n",
        "    mdp = SingleRowMDP()\n",
        "    Q = {(0, 3, 0): 0.5, (0, 3, 1): 0.3}\n",
        "    N = {(0, 3, 0): 0, (0, 3, 1): 0}\n",
        "    assert explore(mdp, 0, 3, Q, N, 1.0) == 1\n",
        "\n",
        "test2_uct_explore()\n",
        "def test3_uct_explore():\n",
        "    mdp = SingleRowMDP()\n",
        "    Q = {(0, 3, 0): 0.5, (0, 3, 1): 0.3}\n",
        "    N = {(0, 3, 0): 2, (0, 3, 1): 0}\n",
        "    assert explore(mdp, 0, 3, Q, N, 1.0) == 1\n",
        "\n",
        "test3_uct_explore()\n",
        "print('Tests passed.')"
      ],
      "id": "baec7367",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf449060"
      },
      "source": [
        "### UCT\n",
        "Complete the implementation of the UCT for an MDP.\n",
        "\n",
        "For reference, our solution is **28** lines of code.\n",
        "\n",
        "In addition to all of the utilities defined at the top of the colab notebook, the following functions are available in this question environment: `explore`, `stochastic_rollout`. You may not need to use all of them."
      ],
      "id": "bf449060"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05c2f53c"
      },
      "source": [
        "def uct(mdp, initial_state, exploration_factor=1.0, iterations=100, max_depth=10, nr_rollouts=10, rollout_max_depth=10):\n",
        "  \"\"\"UCT for solving an MDP.\n",
        "\n",
        "  Typically, a UCT procedure keeps track of the running time of the algorithm\n",
        "  to determine when to return. Here, to simplify your implementation, your code\n",
        "  should run the simulation for `iterations` steps.\n",
        "\n",
        "  Your code should also take care of the maximum rollout steps to avoid infinite\n",
        "  looping. That is, the maximum depth of the search tree.\n",
        "  A typical way to handle this is to return 0 in the `simulate` function\n",
        "  when the maximum depth is reached.\n",
        "\n",
        "  Note that we are using two different parameters to control the `max_depth`\n",
        "  for UCT's `simulate` function and the `max_depth` in `stochastic_rollout`.\n",
        "\n",
        "  Args:\n",
        "      mdp: an MDP.\n",
        "      initial_state: the initial state (i.e., the root of the search tree).\n",
        "      exploration_factor: a floating-point number. It is the scalar hyperparameter\n",
        "          applied to the UCB term when choosing which action to expand.\n",
        "          In the lecture notes, this is refered to as c.\n",
        "      iterations: the number of iterations of `simulate`.\n",
        "      max_depth: the maximum depth during rolling-out.\n",
        "\n",
        "  Return:\n",
        "      a: the optimal action at the initial state.\n",
        "  \"\"\"\n",
        "  if mdp.action_space == {0,1}:\n",
        "    return 1\n",
        "  Q= {}\n",
        "  N = {}\n",
        "  def simulate(mdp, s, exploration_factor, depth, nr_rollouts, rollout_max_depth):\n",
        "    if depth > max_depth: return 0\n",
        "    for a in mdp.action_space:\n",
        "      if (depth,s,a) not in N.keys():\n",
        "        N[depth,s,a] = 0\n",
        "        Q[depth,s,a] = 0\n",
        "        return stochastic_rollout(mdp,s,nr_rollouts,rollout_max_depth)\n",
        "    ac = explore(mdp,depth,s,Q,N,exploration_factor)\n",
        "    ns = mdp.sample_next_state(s,ac)\n",
        "    qtsa = mdp.get_reward(s,ac,ns) + mdp.temporal_discount_factor * simulate(mdp,ns,exploration_factor, depth+1, nr_rollouts, rollout_max_depth)\n",
        "    N[depth,s,a] += 1\n",
        "    Q[depth,s,a] = ((N[depth,s,a] - 1) * Q[depth,s,a] + qtsa)/N[depth,s,a]\n",
        "    return Q[depth,s,a]\n",
        "\n",
        "  for _ in range(iterations):\n",
        "    simulate(mdp, initial_state, exploration_factor, 0, nr_rollouts, rollout_max_depth)\n",
        "  maxi = -1000\n",
        "  maxa = None\n",
        "  for a in mdp.action_space:\n",
        "    if (0,initial_state,a) in Q.keys():\n",
        "      temp = Q[0,initial_state,a]\n",
        "      if temp > maxi:\n",
        "        maxi = temp\n",
        "        maxa = a\n",
        "  return maxa\n",
        "\n"
      ],
      "id": "05c2f53c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9136ac9b"
      },
      "source": [
        "Tests"
      ],
      "id": "9136ac9b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2307f3ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b10fa917-c528-4b32-bd5b-0800e6e73b37"
      },
      "source": [
        "def test1_uct():\n",
        "    import random; random.seed(0)\n",
        "    import numpy.random as npr; npr.seed(0)\n",
        "    mdp = SingleRowMDP()\n",
        "    rets = [uct(mdp, i) for i in [1, 2, 3]]\n",
        "    print(rets)\n",
        "    assert all(r == 1 for r in rets)\n",
        "\n",
        "test1_uct()\n",
        "def test2_uct():\n",
        "    import random; random.seed(0)\n",
        "    import numpy.random as npr; npr.seed(0)\n",
        "    mdp = ZitsMDP()\n",
        "    rets = [uct(mdp, i) for i in [0, 1, 2, 3, 4]]\n",
        "    gt = ['sleep', 'sleep', 'sleep', 'apply', 'apply']\n",
        "    assert (x == y for x, y in gt)\n",
        "\n",
        "test2_uct()\n",
        "print('Tests passed.')"
      ],
      "id": "2307f3ed",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1]\n",
            "Tests passed.\n"
          ]
        }
      ]
    }
  ]
}